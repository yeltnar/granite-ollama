version: "3"
services:
  ollama:
    # add the following lines for desktop GPU
    # devices:
    #  - /dev/dri/card0:/dev/dri/card0 
    image: ollama/ollama
    container_name: ollama
    environment:
      - tmp=tmp
    ports:
      - 11434:11434
    volumes: 
      - $PWD/ollama:/root/.ollama
    # don't restart. this will be managed with systemctl serverless operations 
    # restart: unless-stopped
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports: 
      - 8080:8080 
    volumes: 
      - ./open-web-ui:/app/backend/data 
    environment: OLLAMA_BASE_URL=http://host.containers.internal:11434
    # don't restart. this will be managed with systemctl serverless operations 
    # restart: unless-stopped
  install-models: 
    build: 
      context: . 
      dockerfile: ./install_models.Dockerfile 

